<div class="announce instapaper_body md" data-path="README.md" id="readme"><article class="markdown-body entry-content" itemprop="mainContentOfPage"><h1>
<a name="user-content-deepbeliefsdk" class="anchor" href="#deepbeliefsdk" aria-hidden="true"><span class="octicon octicon-link"></span></a>DeepBeliefSDK</h1>

<p>The SDK for <a href="https://www.jetpac.com">Jetpac's</a> iOS, Android, Linux, and OS X Deep Belief image recognition framework.</p>

<p><a href="https://camo.githubusercontent.com/1492f1227826c0601abc9e5a9ed2679ac50ed334/687474703a2f2f7065746577617264656e2e66696c65732e776f726470726573732e636f6d2f323031342f30342f6c6561726e696e6773686f7436312e706e67" target="_blank"><img src="https://camo.githubusercontent.com/1492f1227826c0601abc9e5a9ed2679ac50ed334/687474703a2f2f7065746577617264656e2e66696c65732e776f726470726573732e636f6d2f323031342f30342f6c6561726e696e6773686f7436312e706e67" alt="" data-canonical-src="http://petewarden.files.wordpress.com/2014/04/learningshot61.png" style="max-width:100%;"></a></p>

<p>This is a  framework implementing the convolutional neural network
architecture <a href="http://www.cs.toronto.edu/%7Ehinton/absps/imagenet.pdf">described by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton</a>.
The processing code has been highly optimized to run within the memory and 
processing constraints of modern mobile devices, and can analyze an image in under 300ms on
an iPhone 5S. It's also easy to use together with OpenCV.</p>

<p>We're releasing this framework because we're excited by the power of
this approach for general image recognition, especially when it can run locally on
low-power devices. It gives your phone the ability to see, and I can't wait to see
what applications that helps you build.</p>

<h3>
<a name="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting started</h3>

<ul class="task-list">
<li><a href="#getting-started-on-ios">iOS</a></li>
<li><a href="#getting-started-on-android">Android</a></li>
<li><a href="#getting-started-on-linux">Linux</a></li>
<li><a href="#getting-started-on-os-x">OS X</a></li>
<li><a href="#getting-started-on-a-raspberry-pi">Raspberry Pi</a></li>
<li><a href="#getting-started-with-javascript">Javascript</a></li>
</ul><h3>
<a name="user-content-adding-to-an-existing-application" class="anchor" href="#adding-to-an-existing-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding to an existing application</h3>

<ul class="task-list">
<li><a href="#adding-to-an-existing-ios-application">iOS</a></li>
<li><a href="#adding-to-an-existing-android-application">Android</a></li>
<li><a href="#adding-to-an-existing-linux-application">Linux</a></li>
<li><a href="#adding-to-an-existing-os-x-application">OS X</a></li>
<li><a href="#using-with-opencv">Using with OpenCV</a></li>
</ul><h3>
<a name="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Documentation</h3>

<ul class="task-list">
<li><a href="#examples">Examples</a></li>
<li><a href="#networks">Networks</a></li>
<li><a href="#api-reference">API Reference</a></li>
<li><a href="#faq">FAQ</a></li>
<li><a href="#more-information">More Information</a></li>
<li><a href="#license">License</a></li>
<li><a href="#credits">Credits</a></li>
</ul><h2>
<a name="user-content-getting-started-on-ios" class="anchor" href="#getting-started-on-ios" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started on iOS</h2>

<p>You'll need the usual tools required for developing iOS applications - XCode 5, an
OS X machine and a modern iOS device (it's been tested as far back as the original
iPhone 4). Open up the SimpleExample/SimpleExample.xcodeproj, build and run.</p>

<p>You should see some warnings (the example is based on Apple sample code which has
some anachronisms in it unfortunately), then once it's running a live camera stream
should be visible on your phone. Move it to look closely at your keyboard, and some
tags should start appearing in the top left of the screen. These should include
things that look like keyboards, including calculators, remote controls, and even
typewriters!</p>

<p>You should experiment with other objects like coffee cups, doors, televisions, and
even dogs if you have any handy! The results will not be human quality, but the 
important part is that they're capturing meaningful attributes of the images.
Understanding images with no context is extremely hard, and while this approach
is a massive step forward compared to the previous state of the art, you'll still
need to adapt it to the domain you're working in to get the best results in a real
application.</p>

<p>Happily the framework includes the ability to retrain the network for custom objects that you care about.
If you have logos you need to pick out, machine parts you need to spot, or just want to be able to distinguish between different kinds of scenes like offices, beaches, mountains or forests, you should look at the <a href="#learningexample">LearningExample</a> sample code.
It builds a custom layer on top of the basic neural network that responds to images you've trained it on, and allows you to embed the functionality in your own application easily.</p>

<p>There's also <a href="https://github.com/jetpacapp/DeepBeliefSDK/wiki/How-to-recognize-custom-objects">this full how-to guide</a> on training and embedding your own custom object recognition code.</p>

<h3>
<a name="user-content-adding-to-an-existing-ios-application" class="anchor" href="#adding-to-an-existing-ios-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding to an existing iOS application</h3>

<p>To use the library in your own application: </p>

<ul class="task-list">
<li>Add the DeepBelief.framework bundle to the Link Binary with Libraries build phase in your XCode project settings.</li>
<li>Add the system Accelerate.framework to your frameworks.</li>
<li>Add <code>#import &lt;DeepBelief/DeepBelief.h&gt;</code> to the top of the file you want to use the code in.</li>
</ul><p>You should then be able to use code like this to classify a single image that you've included as a resource in your bundle. The code assumes it's called 'dog.jpg', but you should change it to match the name of your file.</p>

<div class="highlight highlight-objectivec"><pre>  <span class="bp">NSString</span><span class="o">*</span> <span class="n">networkPath</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">NSBundle</span> <span class="n">mainBundle</span><span class="p">]</span> <span class="nl">pathForResource</span><span class="p">:</span><span class="s">@"jetpac"</span> <span class="nl">ofType</span><span class="p">:</span><span class="s">@"ntwk"</span><span class="p">];</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">networkPath</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"Couldn't find the neural network parameters file - did you add it as a resource to your application?</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">network</span> <span class="o">=</span> <span class="n">jpcnn_create_network</span><span class="p">([</span><span class="n">networkPath</span> <span class="n">UTF8String</span><span class="p">]);</span>
  <span class="n">assert</span><span class="p">(</span><span class="n">network</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>

  <span class="bp">NSString</span><span class="o">*</span> <span class="n">imagePath</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">NSBundle</span> <span class="n">mainBundle</span><span class="p">]</span> <span class="nl">pathForResource</span><span class="p">:</span><span class="s">@"dog"</span> <span class="nl">ofType</span><span class="p">:</span><span class="s">@"jpg"</span><span class="p">];</span>
  <span class="kt">void</span><span class="o">*</span> <span class="n">inputImage</span> <span class="o">=</span> <span class="n">jpcnn_create_image_buffer_from_file</span><span class="p">([</span><span class="n">imagePath</span> <span class="n">UTF8String</span><span class="p">]);</span>

  <span class="kt">float</span><span class="o">*</span> <span class="n">predictions</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">predictionsLength</span><span class="p">;</span>
  <span class="kt">char</span><span class="o">**</span> <span class="n">predictionsLabels</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">predictionsLabelsLength</span><span class="p">;</span>
  <span class="n">jpcnn_classify_image</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">inputImage</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictions</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLength</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLabels</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLabelsLength</span><span class="p">);</span>

  <span class="n">jpcnn_destroy_image_buffer</span><span class="p">(</span><span class="n">inputImage</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="n">predictionsLength</span><span class="p">;</span> <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">predictionValue</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
    <span class="kt">char</span><span class="o">*</span> <span class="n">label</span> <span class="o">=</span> <span class="n">predictionsLabels</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="n">predictionsLabelsLength</span><span class="p">];</span>
    <span class="bp">NSString</span><span class="o">*</span> <span class="n">predictionLine</span> <span class="o">=</span> <span class="p">[</span><span class="bp">NSString</span> <span class="nl">stringWithFormat</span><span class="p">:</span> <span class="s">@"%s - %0.2f</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">predictionValue</span><span class="p">];</span>
    <span class="n">NSLog</span><span class="p">(</span><span class="s">@"%@"</span><span class="p">,</span> <span class="n">predictionLine</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">jpcnn_destroy_network</span><span class="p">(</span><span class="n">network</span><span class="p">);</span>
</pre></div>

<p>If you see errors related to <code>operator new</code> or similar messages at the linking stage, XCode may be skipping the standard C++ library, and that's needed by the DeepBelief.framework code. One workaround I've found is to include an empty .mm or .cpp file in the project to trick XCode into treating it as a C++ project.</p>

<h2>
<a name="user-content-getting-started-on-android" class="anchor" href="#getting-started-on-android" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started on Android</h2>

<p>I've been using Google's ADT toolchain. To get started import the AndroidExample into
their custom version of Eclipse, build and run it. Hopefully you should see a similar
result to the iPhone app, with live video and tags displayed. You'll need to hold the
phone in landscape orientation, look for the tag text and use that as your guide.</p>

<p>The Android implementation uses NEON SIMD instructions, so it may not work on
older phones, and will definitely not work on non-ARM devices. As a benchmark for 
expected performance, classification takes around 650ms on a Samsung Galaxy S5.</p>

<h3>
<a name="user-content-adding-to-an-existing-android-application" class="anchor" href="#adding-to-an-existing-android-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding to an existing Android application</h3>

<p>Under the hood the Android implementation uses a native C++ library that's linked to
Java applications using JNA. That means the process of including the code is a bit more
complex than on iOS. If you look at the <a href="#androidexample">AndroidExample</a> sample code,
you'll see a 'libs' folder. This contains a deepbelief.jar file that has the Java interface
to the underlying native code, and then inside the armeabi there's jnidispatch.so which is
part of JNA and handles the mechanics of calling native functions, and libjpcnn.so which
implements the actual object recognition algorithm. You'll need to replicate this folder
structure and copy the files to your own application's source tree.</p>

<p>Once you've done that, you should be able to import the Java interface to the library:</p>

<p><code>import com.jetpac.deepbelief.DeepBelief.JPCNNLibrary;</code></p>

<p>This class contains a list of Java functions that correspond to exactly to the
<a href="#api-reference">C interface functions</a>. The class code is available in the AndroidLibrary
folder, and you should be able to rebuild it yourself by running ant, but here are the 
definitions using JNA types:</p>

<div class="highlight highlight-java"><pre><span class="n">Pointer</span> <span class="nf">jpcnn_create_network</span><span class="o">(</span><span class="n">String</span> <span class="n">filename</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_destroy_network</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">networkHandle</span><span class="o">);</span>
<span class="n">Pointer</span> <span class="nf">jpcnn_create_image_buffer_from_file</span><span class="o">(</span><span class="n">String</span> <span class="n">filename</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_destroy_image_buffer</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">imageHandle</span><span class="o">);</span>
<span class="n">Pointer</span> <span class="nf">jpcnn_create_image_buffer_from_uint8_data</span><span class="o">(</span><span class="kt">byte</span><span class="o">[]</span> <span class="n">pixelData</span><span class="o">,</span> <span class="kt">int</span> <span class="n">width</span><span class="o">,</span> <span class="kt">int</span> <span class="n">height</span><span class="o">,</span> <span class="kt">int</span> <span class="n">channels</span><span class="o">,</span> <span class="kt">int</span> <span class="n">rowBytes</span><span class="o">,</span> <span class="kt">int</span> <span class="n">reverseOrder</span><span class="o">,</span> <span class="kt">int</span> <span class="n">doRotate</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_classify_image</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">networkHandle</span><span class="o">,</span> <span class="n">Pointer</span> <span class="n">inputHandle</span><span class="o">,</span> <span class="kt">int</span> <span class="n">doMultiSample</span><span class="o">,</span> <span class="kt">int</span> <span class="n">layerOffset</span><span class="o">,</span> <span class="n">PointerByReference</span> <span class="n">outPredictionsValues</span><span class="o">,</span> <span class="n">IntByReference</span> <span class="n">outPredictionsLength</span><span class="o">,</span> <span class="n">PointerByReference</span> <span class="n">outPredictionsNames</span><span class="o">,</span> <span class="n">IntByReference</span> <span class="n">outPredictionsNamesLength</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_print_network</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">networkHandle</span><span class="o">);</span>

<span class="n">Pointer</span> <span class="nf">jpcnn_create_trainer</span><span class="o">();</span>
<span class="kt">void</span> <span class="nf">jpcnn_destroy_trainer</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">trainerHandle</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_train</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">trainerHandle</span><span class="o">,</span> <span class="kt">float</span> <span class="n">expectedLabel</span><span class="o">,</span> <span class="kt">float</span><span class="o">[]</span> <span class="n">predictions</span><span class="o">,</span> <span class="kt">int</span> <span class="n">predictionsLength</span><span class="o">);</span>
<span class="n">Pointer</span> <span class="nf">jpcnn_create_predictor_from_trainer</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">trainerHandle</span><span class="o">);</span>
<span class="kt">void</span> <span class="nf">jpcnn_destroy_predictor</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">predictorHandle</span><span class="o">);</span>
<span class="kt">int</span> <span class="nf">jpcnn_save_predictor</span><span class="o">(</span><span class="n">String</span> <span class="n">filename</span><span class="o">,</span> <span class="n">Pointer</span> <span class="n">predictorHandle</span><span class="o">);</span>
<span class="n">Pointer</span> <span class="nf">jpcnn_load_predictor</span><span class="o">(</span><span class="n">String</span> <span class="n">filename</span><span class="o">);</span>
<span class="kt">float</span> <span class="nf">jpcnn_predict</span><span class="o">(</span><span class="n">Pointer</span> <span class="n">predictorHandle</span><span class="o">,</span> <span class="n">Pointer</span> <span class="n">predictions</span><span class="o">,</span> <span class="kt">int</span> <span class="n">predictionsLength</span><span class="o">);</span>
</pre></div>

<p>There are a few quirks to using the interface that the example code demonstrates how to work around. 
<code>jpcnn_create_network()</code> requires a standard filename path, but to distribute the network with an
application it needs to be an asset, and because that may be compressed and part of an archive, there's
no way to get a path to it. To fix that, <code>initDeepBelief()</code> copys the file to the application's data directory:</p>

<div class="highlight highlight-java"><pre><span class="n">AssetManager</span> <span class="n">am</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="na">getAssets</span><span class="o">();</span>
<span class="n">String</span> <span class="n">baseFileName</span> <span class="o">=</span> <span class="s">"jetpac.ntwk"</span><span class="o">;</span>
<span class="n">String</span> <span class="n">dataDir</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="na">getFilesDir</span><span class="o">().</span><span class="na">getAbsolutePath</span><span class="o">();</span>
<span class="n">String</span> <span class="n">networkFile</span> <span class="o">=</span> <span class="n">dataDir</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">baseFileName</span><span class="o">;</span>
<span class="n">copyAsset</span><span class="o">(</span><span class="n">am</span><span class="o">,</span> <span class="n">baseFileName</span><span class="o">,</span> <span class="n">networkFile</span><span class="o">);</span>
<span class="n">networkHandle</span> <span class="o">=</span> <span class="n">JPCNNLibrary</span><span class="o">.</span><span class="na">INSTANCE</span><span class="o">.</span><span class="na">jpcnn_create_network</span><span class="o">(</span><span class="n">networkFile</span><span class="o">);</span>
</pre></div>

<p>This has some overhead obviously, so one optimization might be to check for the existence of the file and only
copy it over if it doesn't already exist.</p>

<p><code>jpcnn_create_image_buffer_from_uint8_data()</code> needs a plain byte array, and the <code>classifyBitmap()</code> function
shows how you can extract what you need from a normal Bitmap object:</p>

<div class="highlight highlight-java"><pre><span class="kd">final</span> <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">bitmap</span><span class="o">.</span><span class="na">getWidth</span><span class="o">();</span>
<span class="kd">final</span> <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">bitmap</span><span class="o">.</span><span class="na">getHeight</span><span class="o">();</span>
<span class="kd">final</span> <span class="kt">int</span> <span class="n">pixelCount</span> <span class="o">=</span> <span class="o">(</span><span class="n">width</span> <span class="o">*</span> <span class="n">height</span><span class="o">);</span>
<span class="kd">final</span> <span class="kt">int</span> <span class="n">bytesPerPixel</span> <span class="o">=</span> <span class="mi">4</span><span class="o">;</span>
<span class="kd">final</span> <span class="kt">int</span> <span class="n">byteCount</span> <span class="o">=</span> <span class="o">(</span><span class="n">pixelCount</span> <span class="o">*</span> <span class="n">bytesPerPixel</span><span class="o">);</span>
<span class="n">ByteBuffer</span> <span class="n">buffer</span> <span class="o">=</span> <span class="n">ByteBuffer</span><span class="o">.</span><span class="na">allocate</span><span class="o">(</span><span class="n">byteCount</span><span class="o">);</span>
<span class="n">bitmap</span><span class="o">.</span><span class="na">copyPixelsToBuffer</span><span class="o">(</span><span class="n">buffer</span><span class="o">);</span>
<span class="kt">byte</span><span class="o">[]</span> <span class="n">pixels</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="na">array</span><span class="o">();</span>
<span class="n">Pointer</span> <span class="n">imageHandle</span> <span class="o">=</span> <span class="n">JPCNNLibrary</span><span class="o">.</span><span class="na">INSTANCE</span><span class="o">.</span><span class="na">jpcnn_create_image_buffer_from_uint8_data</span><span class="o">(</span><span class="n">pixels</span><span class="o">,</span> <span class="n">width</span><span class="o">,</span> <span class="n">height</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="o">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">width</span><span class="o">),</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">);</span>
</pre></div>

<p>Native objects are not garbage-collected, so you'll have to remember to explicitly call <code>jpcnn_destroy_image_buffer()</code>
and other calls on objects you've created through the library if you want to avoid memory leaks.</p>

<p>The rest of <code>classifyBitmap()</code> also demonstrates how to pull out the results as Java-accessible arrays from the JNA types.</p>

<h2>
<a name="user-content-getting-started-on-linux" class="anchor" href="#getting-started-on-linux" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started on Linux</h2>

<p>I've been using Ubuntu 12.04 and 14.04 on x86-64 platforms, but the library ships as
a simple .so with minimal dependencies, so hopefully it should work on most distros.</p>

<p>As long as you have git and the build-essentials packages installed, you should be able
to build an example by running the following commands in a terminal:</p>

<pre lang="shell"><code>git clone https://github.com/jetpacapp/DeepBeliefSDK.git
cd DeepBeliefSDK/LinuxLibrary
sudo ./install.sh
cd ../examples/SimpleLinux/
make
./deepbelief 
</code></pre>

<p>If <a href="#simplelinux">the example program</a> ran successfully, the output should look like this:</p>

<pre lang="shell"><code>0.016994    wool
0.016418    cardigan
0.010924    kimono
0.010713    miniskirt
0.014307    crayfish
0.015663    brassiere
0.014216    harp
0.017052    sandal
0.024082    holster
0.013580    velvet
0.057286    bonnet
0.018848    stole
0.028298    maillot
0.010915    gown
0.073035    wig
0.012413    hand blower
0.031052    stage
0.027875    umbrella
0.012592    sarong
</code></pre>

<p>It's analyzing the default Lena image, and giving low probabilities of a wig
and a bonnet, which isn't too crazy. You can pass in a command-line argument
to analyze your own images, and the results are tab separated text, so you can
pipe the results into other programs for further processing.</p>

<h3>
<a name="user-content-adding-to-an-existing-linux-application" class="anchor" href="#adding-to-an-existing-linux-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding to an existing Linux application</h3>

<p>To use the library in your own application, first make sure you've run the install.sh script
in AndroidLibrary/ to install the libjpcnn.so in /usr/lib, and libjpcnn.h in /usr/include, as
described in <a href="#getting-started-on-linux">Getting Started on Linux</a>.</p>

<p>Then you should be able to access <a href="#api-reference">all the API functions</a> by including the libjpcnn.h header, eg:</p>

<div class="highlight highlight-c"><pre><span class="cp">#include &lt;libjpcnn.h&gt;</span>
</pre></div>

<p>Here's how you would run a basic classification of a single image, from the <a href="#simplelinux">SimpleLinux example</a>:</p>

<div class="highlight highlight-c"><pre>  <span class="n">networkHandle</span> <span class="o">=</span> <span class="n">jpcnn_create_network</span><span class="p">(</span><span class="n">NETWORK_FILE_NAME</span><span class="p">);</span>
  <span class="n">imageHandle</span> <span class="o">=</span> <span class="n">jpcnn_create_image_buffer_from_file</span><span class="p">(</span><span class="n">imageFileName</span><span class="p">);</span>

  <span class="n">jpcnn_classify_image</span><span class="p">(</span><span class="n">networkHandle</span><span class="p">,</span> <span class="n">imageHandle</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictions</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLength</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLabels</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">predictionsLabelsLength</span><span class="p">);</span>

  <span class="k">for</span> <span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="n">predictionsLength</span><span class="p">;</span> <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">predictionValue</span><span class="p">;</span>
    <span class="kt">char</span><span class="o">*</span> <span class="n">label</span><span class="p">;</span>
    <span class="n">predictionValue</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">predictionValue</span> <span class="o">&lt;</span> <span class="mf">0.01f</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">predictionsLabels</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span> <span class="s">"%f</span><span class="se">\t</span><span class="s">%s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">predictionValue</span><span class="p">,</span> <span class="n">label</span><span class="p">);</span>
  <span class="p">}</span>
</pre></div>

<h2>
<a name="user-content-getting-started-on-os-x" class="anchor" href="#getting-started-on-os-x" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started on OS X</h2>

<p>Load the examples/OSXExample/MyRecorder.xcodeproj XCode project, build, and run.
On any machine with a webcam, you should see a window appear showing live video.
Move the webcam until it has a clear view of an object like a wine bottle, glass, mug, or a computer keyboard, and you should start to see overlaid labels and percentages.</p>

<h3>
<a name="user-content-adding-to-an-existing-os-x-application" class="anchor" href="#adding-to-an-existing-os-x-application" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding to an existing OS X application</h3>

<p>The DeepBelief.framework you'll need is in the OSXLibrary folder.
Since installing frameworks in a shared location can be a pain, and Apple recommends keeping applications as self-contained as possible, it's designed to be bundled inside your app folder.
The <a href="#osxexample">OS X Example</a> sample code uses this approach, and is a good starting point for understanding the process.
It has a symbolic link back to the framework, but you'll probably want to copy the library into your own source tree.
<a href="https://developer.apple.com/library/mac/documentation/macosx/conceptual/BPFrameworks/Tasks/CreatingFrameworks.html#//apple_ref/doc/uid/20002258-106880">Apple's documentation on bundling private frameworks</a> is the best documentation for the whole process, but here's the summary of what you'll need to do:</p>

<ul class="task-list">
<li>Copy DeepBelief.framework into your source tree</li>
<li>Drag it into the Frameworks folder of your project in the XCode navigator.</li>
<li>Add it to the "Link Binary with Libraries" build phase in the project settings.</li>
<li>Add a new "Copy Files Build Phase" to the project build phases.</li>
<li>Add the framework as a new file in that build phase, with the destination as "Frameworks".</li>
</ul><p>Once you've done that, you should be able to build your app, and then "Show package contents" on the built product should show DeepBelief.framework inside the Contents/Frameworks folder.</p>

<p>At that point, just add <code>#import &lt;DeepBelief/DeepBelief.h&gt;</code> and all of the code you need should be identical to the snippets shown in <a href="#adding-to-an-existing-ios-application">the iOS guide</a>.</p>

<h3>
<a name="user-content-using-with-opencv" class="anchor" href="#using-with-opencv" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using with OpenCV</h3>

<p>It's pretty straightforward to use DeepBelief together with OpenCV, you just need to convert the images over.
There's <a href="#simpleopencv">sample code showing the whole process</a>, but the heart of it is this image format conversion:</p>

<div class="highlight highlight-c++"><pre>  <span class="k">const</span> <span class="n">cv</span><span class="o">::</span><span class="n">Size</span> <span class="n">size</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">size</span><span class="p">.</span><span class="n">width</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">size</span><span class="p">.</span><span class="n">height</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">pixelCount</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="n">height</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">bytesPerPixel</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">byteCount</span> <span class="o">=</span> <span class="p">(</span><span class="n">pixelCount</span> <span class="o">*</span> <span class="n">bytesPerPixel</span><span class="p">);</span>

  <span class="c1">// OpenCV images are BGR, we need RGB, so do a conversion to a temporary image</span>
  <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">rgbImage</span><span class="p">;</span>
  <span class="n">cv</span><span class="o">::</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">rgbImage</span><span class="p">,</span> <span class="n">CV_BGR2RGB</span><span class="p">);</span>
  <span class="kt">uint8_t</span><span class="o">*</span> <span class="n">rgbPixels</span> <span class="o">=</span> <span class="p">(</span><span class="kt">uint8_t</span><span class="o">*</span><span class="p">)</span><span class="n">rgbImage</span><span class="p">.</span><span class="n">data</span><span class="p">;</span>

  <span class="n">imageHandle</span> <span class="o">=</span> <span class="n">jpcnn_create_image_buffer_from_uint8_data</span><span class="p">(</span><span class="n">rgbPixels</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">width</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</pre></div>

<p>Once you've done that, you can run the image classification and prediction as normal on the image handle.
<a href="#simpleopencv">The sample code</a> has some other convenience classes too, to help make using the library in C++ a bit easier.
If you're using the Java interface, the same sort of call sequence works to handle the conversion, though you'll need <code>byte[]</code> arrays and you'll have to call <code>image.get(0, 0, pixels)</code> to actually get the raw image data you need.</p>

<h2>
<a name="user-content-getting-started-on-a-raspberry-pi" class="anchor" href="#getting-started-on-a-raspberry-pi" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started on a Raspberry Pi</h2>

<p>The library is available as a Raspbian .so library in the RaspberryPiLibrary folder.
Using it is very similar to ordinary Linux, and you can follow most of the <a href="#getting-started-on-linux">same instructions</a>, substituting the install.sh in the Pi folder.
The biggest difference is that the Pi library uses the GPU to handle a lot of the calculations, so you need to run <a href="#simplelinux">the example program</a> as a super user, e.g. <code>sudo ./deepbelief</code>.
This optimization allows an image to be recognized on a stock Pi in around five seconds, and in three seconds with a boosted GPU clock rate.</p>

<h2>
<a name="user-content-getting-started-with-javascript" class="anchor" href="#getting-started-with-javascript" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting Started with Javascript</h2>

<p>The Javascript version of the library includes complete source, and a <a href="http://jetpacapp.github.io/DeepBeliefSDK/">browser demo page</a>.
The interface is similar to the C version, but uses native Javascript image objects, camelCase function names, and classes.
You'll need to include the jpcnn.js file, and then load the networks/jetpac_untransposed.ntwk file (which is a slightly-modified version of the standard Jetpac network).
Then you should be able to call <code>Network.classifyImage()</code>, with an option to accelerate the calculations using WebGL if you're in a browser that supports it.
On my 2012 MacBook Pro in Chrome, the WebGL version takes around 600ms, whereas the naive CPU path takes 5 seconds.</p>

<h2>
<a name="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples</h2>

<p>All of the sample code projects are included in the 'examples' folder in this git repository.</p>

<ul class="task-list">
<li><a href="#simpleios">SimpleiOS</a></li>
<li><a href="#learningexample">LearningExample</a></li>
<li><a href="#savedmodelexample">SavedModelExample</a></li>
<li><a href="#androidexample">AndroidExample</a></li>
<li><a href="#simplelinux">SimpleLinux</a></li>
<li><a href="#osxexample">OSXExample</a></li>
<li><a href="#simpleopencv">SimpleOpenCV</a></li>
</ul><h3>
<a name="user-content-simpleios" class="anchor" href="#simpleios" aria-hidden="true"><span class="octicon octicon-link"></span></a>SimpleiOS</h3>

<p>This is a self-contained iOS application that shows you how to load the neural network parameters, and process live video to estimate the probability that one of the 1,000 pre-defined Imagenet objects are present.
The code is largely based on the <a href="https://developer.apple.com/library/ios/samplecode/squarecam/Introduction/Intro.html">SquareCam Apple sample application</a>, which is fairly old and contains some ugly code.
If you look for <code>jpcnn_*</code> calls in SquareCamViewController.m you should be able to follow the sequence of first loading the network, applying it to video frames as they arrive, and destroying the objects once you're all done.</p>

<h3>
<a name="user-content-learningexample" class="anchor" href="#learningexample" aria-hidden="true"><span class="octicon octicon-link"></span></a>LearningExample</h3>

<p>This application allows you to apply the image recognition code to custom objects you care about. It demonstrates how to capture positive and negative examples, feed them into a trainer to create a prediction model, and then apply that prediction model to the live camera feed.
It can be a bit messy thanks to all the live video feed code, but if you look for <code>jpcnn_*</code> you'll be able to spot the main flow. Once a prediction model has been fully trained, the parameters are written to the XCode console so they can be used as pre-trained predictors.</p>

<h3>
<a name="user-content-savedmodelexample" class="anchor" href="#savedmodelexample" aria-hidden="true"><span class="octicon octicon-link"></span></a>SavedModelExample</h3>

<p>This shows how you can use a custom prediction model that you've built using the <a href="#learningexample">LearningExample</a> sample code. 
I've included the simple 'wine_bottle_predictor.txt' that I quickly trained on a bottle of wine, you should be able to run it yourself and see the results of that model's prediction on your own images.</p>

<h3>
<a name="user-content-androidexample" class="anchor" href="#androidexample" aria-hidden="true"><span class="octicon octicon-link"></span></a>AndroidExample</h3>

<p>A basic Android application that applies the classification algorithm to live video from the phone's camera. The first thing it does after initialization is analyze the standard image-processing image of Lena, you should see log output from that first.
After that it continuously analyzes incoming camera frames, both displaying the found labels on screen and printing them to the console.</p>

<h3>
<a name="user-content-simplelinux" class="anchor" href="#simplelinux" aria-hidden="true"><span class="octicon octicon-link"></span></a>SimpleLinux</h3>

<p>This is a small command line tool that shows how you can load a network file and classify an image using the default Imagenet categories.
If you run it with no arguments, it looks for lena.png and analyzes that, otherwise it tries to load the file name in the first argument as its input image.</p>

<p>The network file name is hardcoded to "jetpac.ntwk" in the current folder.
In a real application you'll want to set that yourself, either hard-coding it to a known absolute location for the file, or passing it in dynamically as an argument or environment variable.</p>

<p>The output of the tool is tab-separated lines, with the probability first followed by the imagenet label, so you can sort and process it easily through pipes on the command line.</p>

<h3>
<a name="user-content-osxexample" class="anchor" href="#osxexample" aria-hidden="true"><span class="octicon octicon-link"></span></a>OSXExample</h3>

<p>This project is based on <a href="https://developer.apple.com/library/mac/samplecode/MYRecorder/Introduction/Intro.html">Apple's MyRecorder sample code</a>, which is both quite old and fairly gnarly thanks to its use of QTKit!
The complexity is mostly in the way it accesses the webcam, and converts the supplied image down to a simple array of RGB bytes to feed into the neural network code. 
If you search for 'jpcnn' in the code, you'll see the calls to the library nestled amongst all the plumbing for the interface and the video, they should be fairly straightforward.</p>

<p>The main steps are loading the 'jetpac.ntwk' neural network, that's included as a resource in the app, then extracting an image from the video, classifying it, and displaying the found labels in the UI.
When you build and run the project, you should see a window appear with the webcam view in it, and any found labels overlaid on top. You'll also see some performance stats being output to the console - on my mid-2012 Macbook Pro it takes around 60ms to do the calculations.</p>

<h3>
<a name="user-content-simpleopencv" class="anchor" href="#simpleopencv" aria-hidden="true"><span class="octicon octicon-link"></span></a>SimpleOpenCV</h3>

<p>This is a basic Linux command-line tool that shows how OpenCV and the DeepBelief framework can work together.
The main() function uses C++ classes defined in deepbeliefopencv.h to load a network, then it creates an OpenCV image from either lena.png or another file supplied on the command line.
A wrapper class for the library's image handle object is then used to convert the OpenCV image into one the DeepBelief framework can analyze.
The classification is run on that image, and the found labels are printed out.</p>

<p>If you're doing a lot of work with OpenCV, the most crucial part for you is probably the conversion of the image objects between the two systems. 
That's defined in deepbeliefopencv.cpp in the <code>Image::Image(const cv::Mat&amp; image)</code> constructor, and <a href="#using-with-opencv">the section on using OpenCV</a> covers what's going on in the actual code.</p>

<h2>
<a name="user-content-networks" class="anchor" href="#networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Networks</h2>

<p>There are currently three pre-built models available in the networks folder.
jetpac.ntwk is the in-house model used here at Jetpac, and it's licensed under the same BSD conditions as the rest of the project. It has a few oddities, like only 999 labels (a file truncation problem I discovered too late during training) but has served us well and is a good place to start.</p>

<p>The excellent <a href="http://libccv.org">libccv</a> project also made a couple of networks available under a <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. I've converted them over into a binary format, and they're in the networks folder as ccv2010.ntwk and ccv2012.ntwk. You should be able to substitute these in anywhere you'd use jetpac.ntwk. The 2012 file has very similar labels to our original, and the 2010 is an older architecture. You may notice slightly slower performance, the arrangement of the layers is a bit different (in technical terms the local-response normalization happens before the max-pooling in these models, which is more expensive since there's more data to normalize), but the accuracy of the 2012 model especially is good. One common technique in the academic world is to take multiple models and merge their votes for higher accuracy, so one application of the multiple models might be improved accuracy.</p>

<h2>
<a name="user-content-api-reference" class="anchor" href="#api-reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>API Reference</h2>

<p>Because we reuse the same code across a lot of different platforms, we use a
plain-old C interface to our library. All of the handles to different objects are
opaque pointers, and you have to explictly call the <code>*_destroy_*</code> function on any
handles that have been returned from <code>*_create_*</code> calls if you want to avoid memory
leaks. Input images are created from raw arrays of 8-bit RGB data, you can see how 
to build those from iOS types by searching for <code>jpcnn_create_image_buffer()</code> in the 
sample code.</p>

<p>The API is broken up into two sections. The first gives you access to one of the pre-trained neural networks you'll find in the networks folder.
These have been trained on 1,000 Imagenet categories, and the output will give you a decent general idea of what's in an image.</p>

<p>The second section lets you replace the highest layer of the neural network with your own classification step. 
This means you can use it to recognize the objects you care about more accurately.</p>

<h3>
<a name="user-content-pre-trained-calls" class="anchor" href="#pre-trained-calls" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-trained calls</h3>

<ul class="task-list">
<li><a href="#jpcnn_create_network">jpcnn_create_network</a></li>
<li><a href="#jpcnn_destroy_network">jpcnn_destroy_network</a></li>
<li><a href="#jpcnn_create_image_buffer_from_file">jpcnn_create_image_buffer_from_file</a></li>
<li><a href="#jpcnn_create_image_buffer_from_uint8_data">jpcnn_create_image_buffer_from_uint8_data</a></li>
<li><a href="#jpcnn_destroy_image_buffer">jpcnn_destroy_image_buffer</a></li>
<li><a href="#jpcnn_classify_image">jpcnn_classify_image</a></li>
<li><a href="#jpcnn_print_network">jpcnn_print_network</a></li>
</ul><h3>
<a name="user-content-custom-training-calls" class="anchor" href="#custom-training-calls" aria-hidden="true"><span class="octicon octicon-link"></span></a>Custom training calls</h3>

<ul class="task-list">
<li><a href="#jpcnn_create_trainer">jpcnn_create_trainer</a></li>
<li><a href="#jpcnn_destroy_trainer">jpcnn_destroy_trainer</a></li>
<li><a href="#jpcnn_train">jpcnn_train</a></li>
<li><a href="#jpcnn_create_predictor_from_trainer">jpcnn_create_predictor_from_trainer</a></li>
<li><a href="#jpcnn_destroy_predictor">jpcnn_destroy_predictor</a></li>
<li><a href="#jpcnn_load_predictor">jpcnn_load_predictor</a></li>
<li><a href="#jpcnn_print_predictor">jpcnn_print_predictor</a></li>
<li><a href="#jpcnn_predict">jpcnn_predict</a></li>
</ul><h3>
<a name="user-content-jpcnn_create_network" class="anchor" href="#jpcnn_create_network" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_create_network</h3>

<p><code>void* jpcnn_create_network(const char* filename)</code></p>

<p>This takes the filename of the network parameter file as an input, and builds a 
neural network stack based on that definition. Right now the only available file
is the 1,000 category jetpac.ntwk, built here at Jetpac based on the
approach used by Krizhevsky to win the Imagenet 2012 competition.</p>

<p>You'll need to make sure you include this 60MB file in the 'Copy Files' build phase
of your application, and then call something like this to get the actual path:</p>

<pre><code>NSString* networkPath = [[NSBundle mainBundle] pathForResource:@"jetpac" ofType:@"ntwk"];
network = jpcnn_create_network([networkPath UTF8String]);
</code></pre>

<h3>
<a name="user-content-jpcnn_destroy_network" class="anchor" href="#jpcnn_destroy_network" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_destroy_network</h3>

<p><code>void jpcnn_destroy_network(void* networkHandle)</code></p>

<p>Once you're finished with the neural network, call this to destroy it and free
up the memory it used.</p>

<h3>
<a name="user-content-jpcnn_create_image_buffer_from_file" class="anchor" href="#jpcnn_create_image_buffer_from_file" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_create_image_buffer_from_file</h3>

<p><code>void* jpcnn_create_image_buffer_from_file(const char* filename)</code></p>

<p>Takes a filename (see above for how to get one from your bundle) and creates an
image object that you can run the classification process on. It can load PNGS and
JPEGS.</p>

<h3>
<a name="user-content-jpcnn_create_image_buffer_from_uint8_data" class="anchor" href="#jpcnn_create_image_buffer_from_uint8_data" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_create_image_buffer_from_uint8_data</h3>

<p><code>void* jpcnn_create_image_buffer_from_uint8_data(unsigned char* pixelData, int width, int height, int channels, int rowBytes, int reverseOrder, int doRotate)</code></p>

<p>If you already have data in memory, you can use this function to copy it into an
image object that you can then classify. It's useful if you're doing video capture,
as the sample code does.</p>

<h3>
<a name="user-content-jpcnn_destroy_image_buffer" class="anchor" href="#jpcnn_destroy_image_buffer" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_destroy_image_buffer</h3>

<p><code>void jpcnn_destroy_image_buffer(void* imageHandle)</code></p>

<p>Once you're done classifying an image, call this to free up the memory it used.</p>

<h3>
<a name="user-content-jpcnn_classify_image" class="anchor" href="#jpcnn_classify_image" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_classify_image</h3>

<p><code>void jpcnn_classify_image(void* networkHandle, void* inputHandle, unsigned int flags, int layerOffset, float** outPredictionsValues, int* outPredictionsLength, char*** outPredictionsNames, int* outPredictionsNamesLength)</code></p>

<p>This is how you actually get tags for an image. It takes in a neural network and an
image, and returns an array of floats. Each float is a predicted value for an
imagenet label, between 0 and 1, where higher numbers are more confident predictions.</p>

<p>The three outputs are:</p>

<ul class="task-list">
<li>outPredictionsValues is a pointer to the array of predictions.</li>
<li>outPredictionsLength holds the length of the predictions array.</li>
<li>outPredictionsNames is an array of C strings representing imagenet labels, each corresponding to the prediction value at the same index in outPredictionsValues.</li>
<li>outPredictionsNamesLength is the number of name strings in the label array. In the simple case this is the same as the number of predictions, but in different modes this can get more complicated! See below for details.</li>
</ul><p>In the simple case you can leave the flags and layerOffset arguments as zero, and you'll get an array of prediction values out. Pick the highest (possibly with a threshold like 0.1 to avoid shaky ones), and you can use that as a simple tag for the image.</p>

<p>There are several optional arguments you can use to improve your results though.</p>

<h4>
<a name="user-content-layeroffset" class="anchor" href="#layeroffset" aria-hidden="true"><span class="octicon octicon-link"></span></a>layerOffset</h4>

<p>The final output of the neural network represents the high-level categories that it's been trained on, but often you'll want to work with other types of objects.
The good news is that it's possible to take the results from layers that are just before the final one, and use those as inputs to simple statistical algorithms to recognize entirely new kinds of things.
<a href="http://arxiv.org/abs/1310.1531">This paper on Decaf</a> does a good job of describing the approach, but the short version is that those high-level layers can be seen as adjectives that help the output layer make its final choice between categories, and those same adjectives turn out to be useful for choosing between a lot of other categories it hasn't been trained on too.
For example, there might be some signals that correlate with 'spottiness' and 'furriness', which would be useful for picking out leopards, even if they were originally learned from pictures of dalmatians.</p>

<p>The <code>layerOffset</code> argument lets you control which layer you're sampling, as a negative offset from the start of the network. 
Try setting it to <code>-2</code>, and you should get an array of 4096 floats in outPredictionsValues, though since these are no longer representing Imagenet labels the names array will no longer be valid. 
You can then feed those values into a training system like libSVM to help you distinguish between the kinds of objects you care about.</p>

<h4>
<a name="user-content-flags" class="anchor" href="#flags" aria-hidden="true"><span class="octicon octicon-link"></span></a>flags</h4>

<p>The image recognition algorithm always crops the input image to the biggest square that fits within its bounds, resamples that area to 256x256 pixels and then takes a slightly smaller 224x224 sample square from somewhere within that main square.
The flags argument controls how that 224-pixel sample square is positioned within the larger one. If it's left as zero, then it's centered with a 16 pixel margin at all edges. 
The sample code uses <code>JPCNN_RANDOM_SAMPLE</code> to jitter the origin of the 224 square randomly within the bounds each call, since this, combined with smoothing of the results over time, helps ensure that the identification of tags is robust to slight position changes.
The <code>JPCNN_MULTISAMPLE</code> flag takes ten different sample positions within the image and runs them all through the classification pipeline simultaneously. This is a costly operation, so it doesn't tend to be practical on low-processing-power platforms like the iPhone.</p>

<h3>
<a name="user-content-jpcnn_print_network" class="anchor" href="#jpcnn_print_network" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_print_network</h3>

<p><code>void jpcnn_print_network(void* networkHandle)</code></p>

<p>This is a debug logging call that prints information about a loaded neural network.</p>

<h3>
<a name="user-content-jpcnn_create_trainer" class="anchor" href="#jpcnn_create_trainer" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_create_trainer</h3>

<p><code>void* jpcnn_create_trainer()</code></p>

<p>Returns a handle to a trainer object that you can feed training examples into to build your own custom prediction model.</p>

<h3>
<a name="user-content-jpcnn_destroy_trainer" class="anchor" href="#jpcnn_destroy_trainer" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_destroy_trainer</h3>

<p><code>void jpcnn_destroy_trainer(void* trainerHandle)</code></p>

<p>Disposes of the memory used by the trainer object and destroys it.</p>

<h3>
<a name="user-content-jpcnn_train" class="anchor" href="#jpcnn_train" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_train</h3>

<p><code>void jpcnn_train(void* trainerHandle, float expectedLabel, float* predictions, int predictionsLength)</code></p>

<p>To create your own custom prediction model, you need to train it using 'positive' examples of images containing the object you care about, and 'negative' examples of images that don't.
Once you've created a trainer object, you can call this with the neural network results for each positive or negative image, and with an expectedLabel of '0.0' for negatives and '1.0' for positives.
Picking the exact number of each you'll need is more of an art than a science, since it depends on how easy your object is to recognize and how cluttered your environment is, but I've had decent results with as few as a hundred of each.
You can use the output of any layer of the neural network, but I've found using the penultimate one works well. I discuss how to do this above in the <a href="#layeroffset">layerOffset</a> section.
To see how this works in practice, try out the <a href="#learningexample">LearningExample</a> sample code for yourself.</p>

<h3>
<a name="user-content-jpcnn_create_predictor_from_trainer" class="anchor" href="#jpcnn_create_predictor_from_trainer" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_create_predictor_from_trainer</h3>

<p><code>void* jpcnn_create_predictor_from_trainer(void* trainerHandle)</code></p>

<p>Once you've passed in all your positive and negative examples to <a href="#jpcnn_train">jpcnn_train</a>, you can call this to build a predictor model from them all. 
Under the hood, it's using libSVM to create a support vector machine model based on the examples.</p>

<h3>
<a name="user-content-jpcnn_destroy_predictor" class="anchor" href="#jpcnn_destroy_predictor" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_destroy_predictor</h3>

<p><code>void jpcnn_destroy_predictor(void* predictorHandle)</code></p>

<p>Deallocates any memory used by the predictor model, call this once you're finished with it.</p>

<h3>
<a name="user-content-jpcnn_load_predictor" class="anchor" href="#jpcnn_load_predictor" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_load_predictor</h3>

<p><code>void* jpcnn_load_predictor(const char* filename)</code></p>

<p>Loads a predictor you've already created from a libSVM-format text file. 
Since you can't save files on iOS devices, the only way to create this file in the first place is to call <a href="#jpcnn_print_predictor">jpcnn_print_predictor</a> once you've created a predictor, and then copy and paste the results from the developer console into a file, and then add it to your app's resources. 
The <a href="#savedmodelExample">SavedModelExample</a> sample code shows how to use this call.</p>

<h3>
<a name="user-content-jpcnn_print_predictor" class="anchor" href="#jpcnn_print_predictor" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_print_predictor</h3>

<p><code>void jpcnn_print_predictor(void* predictorHandle)</code></p>

<p>Outputs the parameters that define a custom predictor to stderr (and hence the developer console in XCode). You'll need to copy and paste this into your own text file to subsequently reload the predictor.</p>

<h3>
<a name="user-content-jpcnn_predict" class="anchor" href="#jpcnn_predict" aria-hidden="true"><span class="octicon octicon-link"></span></a>jpcnn_predict</h3>

<p><code>float jpcnn_predict(void* predictorHandle, float* predictions, int predictionsLength)</code></p>

<p>Given the output from a pre-trained neural network, and a custom prediction model, returns a value estimating the probability that the image contains the object it has been trained against.</p>

<h2>
<a name="user-content-faq" class="anchor" href="#faq" aria-hidden="true"><span class="octicon octicon-link"></span></a>FAQ</h2>

<h3>
<a name="user-content-is-this-available-for-platforms-other-than-ios-android-os-x-and-linux-x86-64" class="anchor" href="#is-this-available-for-platforms-other-than-ios-android-os-x-and-linux-x86-64" aria-hidden="true"><span class="octicon octicon-link"></span></a>Is this available for platforms other than iOS, Android, OS X, and Linux x86-64?</h3>

<p>Not right now. I hope to make it available on other devices like the Raspberry Pi in the future. I recommend checking out <a href="https://github.com/BVLC/caffe">Caffe</a>, <a href="http://cilvr.nyu.edu/doku.php?id=software:overfeat:start">OverFeat</a> and <a href="http://libccv.org">libCCV</a> if you're on the desktop too, they're great packages.</p>

<h3>
<a name="user-content-is-the-source-available" class="anchor" href="#is-the-source-available" aria-hidden="true"><span class="octicon octicon-link"></span></a>Is the source available?</h3>

<p>Not at the moment. The compiled library and the neural network parameter set are freely reusable in your own apps under the BSD license though.</p>

<h3>
<a name="user-content-can-i-train-my-own-networks" class="anchor" href="#can-i-train-my-own-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Can I train my own networks?</h3>

<p>There aren't any standard formats for sharing large neural networks unfortunately, so there's no easy way to import other CNNs into the app. The <a href="https://github.com/jetpacapp/DeepBeliefSDK/wiki/How-to-recognize-custom-objects">custom training</a> should help you apply the included pre-trained network to your own problems to a large extent though.</p>

<h2>
<a name="user-content-more-information" class="anchor" href="#more-information" aria-hidden="true"><span class="octicon octicon-link"></span></a>More Information</h2>

<p>Join the <a href="https://groups.google.com/group/deep-belief-developers">Deep Belief Developers email list</a> to find out more about the practical details of implementing deep learning.</p>

<h2>
<a name="user-content-license" class="anchor" href="#license" aria-hidden="true"><span class="octicon octicon-link"></span></a>License</h2>

<p>The binary framework and jetpac.ntwk network parameter file are under the BSD three-clause
license, included in this folder as LICENSE. All source code is under that BSD license unless otherwise noted.</p>

<p>The ccv2010.ntwk and ccv2012.ntwk network models were converted from files created as part of the <a href="http://libccv.org/">LibCCV project</a> and are licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</a>.</p>

<h2>
<a name="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span class="octicon octicon-link"></span></a>Credits</h2>

<p>Big thanks go to:</p>

<ul class="task-list">
<li>
<a href="http://danielnouri.org/">Daniel Nouri</a> for his invaluable help on CNNs.</li>
<li>
<a href="http://www.cs.toronto.edu/%7Ehinton/absps/imagenet.pdf">Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton</a> for their ground-breaking work on ConvNet.</li>
<li>
<a href="https://plus.google.com/+YannLeCunPhD/posts">Yann LeCun</a> for his pioneering research and continuing support of the field.</li>
<li>The Berkeley Vision and Learning Center for their work on <a href="https://github.com/BVLC/caffe">lightweight custom training of CNNs</a>.</li>
<li>My colleagues <a href="https://twitter.com/lindblomsten">Cathrine</a>, <a href="https://twitter.com/davefearon">Dave</a>, <a href="https://twitter.com/juliangreensf">Julian</a>, and <a href="https://twitter.com/spara">Sophia</a> at <a href="https://www.jetpac.com/">Jetpac</a> for all their help.</li>
</ul><p><a href="https://twitter.com/petewarden">Pete Warden</a></p></article></div>