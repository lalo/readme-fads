<div class="announce instapaper_body md" data-path="README.md" id="readme"><article class="markdown-body entry-content" itemprop="mainContentOfPage"><h1>
<a name="user-content-jieba" class="anchor" href="#jieba" aria-hidden="true"><span class="octicon octicon-link"></span></a>jieba</h1>

<p>"结巴"中文分词：做最好的Python中文分词组件
"Jieba" (Chinese for "to stutter") Chinese text segmentation: built to be the best Python Chinese word segmentation module.</p>

<ul class="task-list">
<li><em>Scroll down for English documentation.</em></li>
</ul><h1>
<a name="user-content-feature" class="anchor" href="#feature" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature</h1>

<ul class="task-list">
<li>
<p>支持三种分词模式：</p>

<ul class="task-list">
<li>精确模式，试图将句子最精确地切开，适合文本分析；</li>
<li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li>
<li>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li>
</ul>
</li>
<li><p>支持繁体分词</p></li>
<li>支持自定义词典</li>
</ul><h1>
<a name="user-content-%E5%9C%A8%E7%BA%BF%E6%BC%94%E7%A4%BA" class="anchor" href="#%E5%9C%A8%E7%BA%BF%E6%BC%94%E7%A4%BA" aria-hidden="true"><span class="octicon octicon-link"></span></a>在线演示</h1>

<p><a href="http://jiebademo.ap01.aws.af.cm/">http://jiebademo.ap01.aws.af.cm/</a></p>

<p>(Powered by Appfog)</p>

<p>网站代码：<a href="https://github.com/fxsjy/jiebademo">https://github.com/fxsjy/jiebademo</a></p>

<h1>
<a name="user-content-python-2x-%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85" class="anchor" href="#python-2x-%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85" aria-hidden="true"><span class="octicon octicon-link"></span></a>Python 2.x 下的安装</h1>

<ul class="task-list">
<li>全自动安装：<code>easy_install jieba</code> 或者 <code>pip install jieba</code>
</li>
<li>半自动安装：先下载<a href="http://pypi.python.org/pypi/jieba/">http://pypi.python.org/pypi/jieba/</a> ，解压后运行python setup.py install</li>
<li>手动安装：将jieba目录放置于当前目录或者site-packages目录</li>
<li>通过import jieba 来引用</li>
</ul><h1>
<a name="user-content-python-3x-%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85" class="anchor" href="#python-3x-%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85" aria-hidden="true"><span class="octicon octicon-link"></span></a>Python 3.x 下的安装</h1>

<ul class="task-list">
<li>目前master分支是只支持Python2.x 的</li>
<li>
<p>Python3.x 版本的分支也已经基本可用： <a href="https://github.com/fxsjy/jieba/tree/jieba3k">https://github.com/fxsjy/jieba/tree/jieba3k</a></p>

<pre><code>git clone https://github.com/fxsjy/jieba.git
git checkout jieba3k
python setup.py install
</code></pre>
</li>
</ul><h1>
<a name="user-content-%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Djava%E7%89%88%E6%9C%AC" class="anchor" href="#%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Djava%E7%89%88%E6%9C%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>结巴分词Java版本</h1>

<p>作者：piaolingxue
地址：<a href="https://github.com/huaban/jieba-analysis">https://github.com/huaban/jieba-analysis</a></p>

<h1>
<a name="user-content-%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Dc%E7%89%88%E6%9C%AC" class="anchor" href="#%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Dc%E7%89%88%E6%9C%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>结巴分词C++版本</h1>

<p>作者：Aszxqw
地址：<a href="https://github.com/aszxqw/cppjieba">https://github.com/aszxqw/cppjieba</a></p>

<h1>
<a name="user-content-%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Dnodejs%E7%89%88%E6%9C%AC" class="anchor" href="#%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Dnodejs%E7%89%88%E6%9C%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>结巴分词Node.js版本</h1>

<p>作者：Aszxqw
地址：<a href="https://github.com/aszxqw/nodejieba">https://github.com/aszxqw/nodejieba</a></p>

<h1>
<a name="user-content-%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Derlang%E7%89%88%E6%9C%AC" class="anchor" href="#%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8Derlang%E7%89%88%E6%9C%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>结巴分词Erlang版本</h1>

<p>作者：falood
<a href="https://github.com/falood/exjieba">https://github.com/falood/exjieba</a></p>

<h1>
<a name="user-content-algorithm" class="anchor" href="#algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm</h1>

<ul class="task-list">
<li>基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li>
<li>对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法</li>
</ul><h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-1%E5%88%86%E8%AF%8D" class="anchor" href="#%E5%8A%9F%E8%83%BD-1%E5%88%86%E8%AF%8D" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 1)：分词</h1>

<ul class="task-list">
<li>
<code>jieba.cut</code>方法接受两个输入参数: 1) 第一个参数为需要分词的字符串 2）cut_all参数用来控制是否采用全模式</li>
<li>
<code>jieba.cut_for_search</code>方法接受一个参数：需要分词的字符串,该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细</li>
<li>注意：待分词的字符串可以是gbk字符串、utf-8字符串或者unicode</li>
<li>
<code>jieba.cut</code>以及<code>jieba.cut_for_search</code>返回的结构都是一个可迭代的generator，可以使用for循环来获得分词后得到的每一个词语(unicode)，也可以用list(jieba.cut(...))转化为list</li>
</ul><p>代码示例( 分词 )</p>

<pre><code>#encoding=utf-8
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print "Full Mode:", "/ ".join(seg_list)  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print "Default Mode:", "/ ".join(seg_list)  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print ", ".join(seg_list)

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print ", ".join(seg_list)
</code></pre>

<p>Output:</p>

<pre><code>【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学

【精确模式】: 我/ 来到/ 北京/ 清华大学

【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)

【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造
</code></pre>

<h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-2-%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%8D%E5%85%B8" class="anchor" href="#%E5%8A%9F%E8%83%BD-2-%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%8D%E5%85%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 2) ：添加自定义词典</h1>

<ul class="task-list">
<li>开发者可以指定自己自定义的词典，以便包含jieba词库里没有的词。虽然jieba有新词识别能力，但是自行添加新词可以保证更高的正确率</li>
<li>用法： jieba.load_userdict(file_name) # file_name为自定义词典的路径</li>
<li>词典格式和<code>dict.txt</code>一样，一个词占一行；每一行分三部分，一部分为词语，另一部分为词频，最后为词性（可省略），用空格隔开</li>
<li>
<p>范例：</p>

<ul class="task-list">
<li><p>自定义词典：<a href="https://github.com/fxsjy/jieba/blob/master/test/userdict.txt">https://github.com/fxsjy/jieba/blob/master/test/userdict.txt</a></p></li>
<li>
<p>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py">https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py</a></p>

<ul class="task-list">
<li><p>之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /</p></li>
<li><p>加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /</p></li>
</ul>
</li>
</ul>
</li>
<li><p>"通过用户自定义词典来增强歧义纠错能力" --- <a href="https://github.com/fxsjy/jieba/issues/14">https://github.com/fxsjy/jieba/issues/14</a></p></li>
</ul><h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-3-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96" class="anchor" href="#%E5%8A%9F%E8%83%BD-3-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 3) ：关键词提取</h1>

<ul class="task-list">
<li>jieba.analyse.extract_tags(sentence,topK) #需要先import jieba.analyse</li>
<li>setence为待提取的文本</li>
<li>topK为返回几个TF/IDF权重最大的关键词，默认值为20</li>
</ul><p>代码示例 （关键词提取）</p>

<pre><code>https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py
</code></pre>

<h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-4--%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8" class="anchor" href="#%E5%8A%9F%E8%83%BD-4--%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 4) : 词性标注</h1>

<ul class="task-list">
<li>标注句子分词后每个词的词性，采用和ictclas兼容的标记法</li>
<li>
<p>用法示例</p>

<pre><code>&gt;&gt;&gt; import jieba.posseg as pseg
&gt;&gt;&gt; words = pseg.cut("我爱北京天安门")
&gt;&gt;&gt; for w in words:
...    print w.word, w.flag
...
我 r
爱 v
北京 ns
天安门 ns
</code></pre>
</li>
</ul><h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-5--%E5%B9%B6%E8%A1%8C%E5%88%86%E8%AF%8D" class="anchor" href="#%E5%8A%9F%E8%83%BD-5--%E5%B9%B6%E8%A1%8C%E5%88%86%E8%AF%8D" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 5) : 并行分词</h1>

<ul class="task-list">
<li>原理：将目标文本按行分隔后，把各行文本分配到多个python进程并行分词，然后归并结果，从而获得分词速度的可观提升</li>
<li>基于python自带的multiprocessing模块，目前暂不支持windows</li>
<li>
<p>用法：</p>

<ul class="task-list">
<li>
<code>jieba.enable_parallel(4)</code> # 开启并行分词模式，参数为并行进程数</li>
<li>
<code>jieba.disable_parallel()</code> # 关闭并行分词模式</li>
</ul>
</li>
<li><p>例子：
    <a href="https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py">https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py</a></p></li>
<li><p>实验结果：在4核3.4GHz Linux机器上，对金庸全集进行精确分词，获得了1MB/s的速度，是单进程版的3.3倍。</p></li>
</ul><h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-6--tokenize%E8%BF%94%E5%9B%9E%E8%AF%8D%E8%AF%AD%E5%9C%A8%E5%8E%9F%E6%96%87%E7%9A%84%E8%B5%B7%E5%A7%8B%E4%BD%8D%E7%BD%AE" class="anchor" href="#%E5%8A%9F%E8%83%BD-6--tokenize%E8%BF%94%E5%9B%9E%E8%AF%8D%E8%AF%AD%E5%9C%A8%E5%8E%9F%E6%96%87%E7%9A%84%E8%B5%B7%E5%A7%8B%E4%BD%8D%E7%BD%AE" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 6) : Tokenize：返回词语在原文的起始位置</h1>

<ul class="task-list">
<li>注意，输入参数只接受unicode</li>
<li>默认模式</li>
</ul><div class="highlight highlight-python"><pre><span class="n">result</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">u'永和服装饰品有限公司'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">tk</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">"word </span><span class="si">%s</span><span class="se">\t\t</span><span class="s"> start: </span><span class="si">%d</span><span class="s"> </span><span class="se">\t\t</span><span class="s"> end:</span><span class="si">%d</span><span class="s">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">tk</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">tk</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">tk</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>

<pre><code>word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限公司            start: 6                end:10

</code></pre>

<ul class="task-list">
<li>搜索模式</li>
</ul><div class="highlight highlight-python"><pre><span class="n">result</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">u'永和服装饰品有限公司'</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s">'search'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">tk</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">"word </span><span class="si">%s</span><span class="se">\t\t</span><span class="s"> start: </span><span class="si">%d</span><span class="s"> </span><span class="se">\t\t</span><span class="s"> end:</span><span class="si">%d</span><span class="s">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">tk</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">tk</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">tk</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>

<pre><code>word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限                start: 6                end:8
word 公司                start: 8                end:10
word 有限公司            start: 6                end:10
</code></pre>

<h1>
<a name="user-content-%E5%8A%9F%E8%83%BD-7--chineseanalyzer-for-whoosh%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E" class="anchor" href="#%E5%8A%9F%E8%83%BD-7--chineseanalyzer-for-whoosh%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E" aria-hidden="true"><span class="octicon octicon-link"></span></a>功能 7) : ChineseAnalyzer for Whoosh搜索引擎</h1>

<ul class="task-list">
<li>引用： <code>from jieba.analyse import ChineseAnalyzer</code>
</li>
<li>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py">https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py</a>
</li>
</ul><h1>
<a name="user-content-%E5%85%B6%E4%BB%96%E8%AF%8D%E5%85%B8" class="anchor" href="#%E5%85%B6%E4%BB%96%E8%AF%8D%E5%85%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>其他词典</h1>

<ol class="task-list">
<li><p>占用内存较小的词典文件
<a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small</a></p></li>
<li><p>支持繁体分词更好的词典文件
<a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big</a></p></li>
</ol><p>下载你所需要的词典，然后覆盖jieba/dict.txt 即可或者用<code>jieba.set_dictionary('data/dict.txt.big')</code></p>

<h1>
<a name="user-content-%E6%A8%A1%E5%9D%97%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9C%BA%E5%88%B6%E7%9A%84%E6%94%B9%E5%8F%98lazy-load-%E4%BB%8E028%E7%89%88%E6%9C%AC%E5%BC%80%E5%A7%8B" class="anchor" href="#%E6%A8%A1%E5%9D%97%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9C%BA%E5%88%B6%E7%9A%84%E6%94%B9%E5%8F%98lazy-load-%E4%BB%8E028%E7%89%88%E6%9C%AC%E5%BC%80%E5%A7%8B" aria-hidden="true"><span class="octicon octicon-link"></span></a>模块初始化机制的改变:lazy load （从0.28版本开始）</h1>

<p>jieba采用延迟加载，"import jieba"不会立即触发词典的加载，一旦有必要才开始加载词典构建trie。如果你想手工初始jieba，也可以手动初始化。</p>

<pre><code>import jieba
jieba.initialize()  # 手动初始化（可选）
</code></pre>

<p>在0.28之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径:</p>

<pre><code>jieba.set_dictionary('data/dict.txt.big')
</code></pre>

<p>例子： <a href="https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py">https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py</a></p>

<h1>
<a name="user-content-%E5%88%86%E8%AF%8D%E9%80%9F%E5%BA%A6" class="anchor" href="#%E5%88%86%E8%AF%8D%E9%80%9F%E5%BA%A6" aria-hidden="true"><span class="octicon octicon-link"></span></a>分词速度</h1>

<ul class="task-list">
<li>1.5 MB / Second in Full Mode</li>
<li>400 KB / Second in Default Mode</li>
<li>Test Env: Intel(R) Core(TM) i7-2600 CPU @ 3.4GHz；《围城》.txt</li>
</ul><h1>
<a name="user-content-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98" class="anchor" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>常见问题</h1>

<p>1）模型的数据是如何生成的？<a href="https://github.com/fxsjy/jieba/issues/7">https://github.com/fxsjy/jieba/issues/7</a></p>

<p>2）这个库的授权是? <a href="https://github.com/fxsjy/jieba/issues/2">https://github.com/fxsjy/jieba/issues/2</a></p>

<p>更多问题请点击：<a href="https://github.com/fxsjy/jieba/issues?sort=updated&amp;state=closed">https://github.com/fxsjy/jieba/issues?sort=updated&amp;state=closed</a></p>

<h1>
<a name="user-content-change-log" class="anchor" href="#change-log" aria-hidden="true"><span class="octicon octicon-link"></span></a>Change Log</h1>

<p><a href="https://github.com/fxsjy/jieba/blob/master/Changelog">https://github.com/fxsjy/jieba/blob/master/Changelog</a></p>

<h1>
<a name="user-content-jieba-1" class="anchor" href="#jieba-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>jieba</h1>

<p>"Jieba" (Chinese for "to stutter") Chinese text segmentation: built to be the best Python Chinese word segmentation module.</p>

<h1>
<a name="user-content-features" class="anchor" href="#features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features</h1>

<ul class="task-list">
<li>Support three types of segmentation mode:</li>
<li>1) Accurate Mode, attempt to cut the sentence into the most accurate segmentation, which is suitable for text analysis;</li>
<li>2) Full Mode, break the words of the sentence into words scanned</li>
<li>3) Search Engine Mode, based on the Accurate Mode, with an attempt to cut the long words into several short words, which can enhance the recall rate</li>
</ul><h1>
<a name="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h1>

<ul class="task-list">
<li>Fully automatic installation: <code>easy_install jieba</code> or <code>pip install jieba</code>
</li>
<li>Semi-automatic installation: Download <a href="http://pypi.python.org/pypi/jieba/">http://pypi.python.org/pypi/jieba/</a> , after extracting run <code>python setup.py install</code>
</li>
<li>Manutal installation: place the <code>jieba</code> directory in the current directory or python site-packages directory.</li>
<li>Use <code>import jieba</code> to import, which will first build the Trie tree only on first import (takes a few seconds).</li>
</ul><h1>
<a name="user-content-algorithm-1" class="anchor" href="#algorithm-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm</h1>

<ul class="task-list">
<li>Based on the Trie tree structure to achieve efficient word graph scanning; sentences using Chinese characters constitute a directed acyclic graph (DAG)</li>
<li>Employs memory search to calculate the maximum probability path, in order to identify the maximum tangential points based on word frequency combination</li>
<li>For unknown words, the character position HMM-based model is used, using the Viterbi algorithm</li>
</ul><h1>
<a name="user-content-function-1-cut" class="anchor" href="#function-1-cut" aria-hidden="true"><span class="octicon octicon-link"></span></a>Function 1): cut</h1>

<ul class="task-list">
<li>The <code>jieba.cut</code> method accepts to input parameters: 1) the first parameter is the string that requires segmentation, and the 2) second parameter is <code>cut_all</code>, a parameter used to control the segmentation pattern.</li>
<li>
<code>jieba.cut</code> returned structure is an iterative generator, where you can use a <code>for</code> loop to get the word segmentation (in unicode), or <code>list(jieba.cut( ... ))</code> to create a list.</li>
<li>
<code>jieba.cut_for_search</code> accpets only on parameter: the string that requires segmentation, and it will cut the sentence into short words</li>
</ul><h1>
<a name="user-content-code-example-segmentation" class="anchor" href="#code-example-segmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Code example: segmentation</h1>

<pre><code>#encoding=utf-8
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print "Full Mode:", "/ ".join(seg_list)  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print "Default Mode:", "/ ".join(seg_list)  # 默认模式

seg_list = jieba.cut("他来到了网易杭研大厦")
print ", ".join(seg_list)

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print ", ".join(seg_list)
</code></pre>

<p>Output:</p>

<pre><code>[Full Mode]: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学

[Accurate Mode]: 我/ 来到/ 北京/ 清华大学

[Unknown Words Recognize] 他, 来到, 了, 网易, 杭研, 大厦    (In this case, "杭研" is not in the dictionary, but is identified by the Viterbi algorithm)

[Search Engine Mode]： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在
</code></pre>

<p>, 日本, 京都, 大学, 日本京都大学, 深造</p>

<h1>
<a name="user-content-function-2-add-a-custom-dictionary" class="anchor" href="#function-2-add-a-custom-dictionary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Function 2): Add a custom dictionary</h1>

<ul class="task-list">
<li>Developers can specify their own custom dictionary to include in the jieba thesaurus. jieba has the ability to identify new words, but adding your own new words can ensure a higher rate of correct segmentation.</li>
<li>Usage： <code>jieba.load_userdict(file_name) # file_name is a custom dictionary path</code>
</li>
<li>The dictionary format is the same as that of <code>analyse/idf.txt</code>: one word per line; each line is divided into two parts, the first is the word itself, the other is the word frequency, separated by a space</li>
<li>
<p>Example：</p>

<pre><code>云计算 5
李小福 2
创新办 3

之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /

加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /
</code></pre>
</li>
</ul><h1>
<a name="user-content-function-3-keyword-extraction" class="anchor" href="#function-3-keyword-extraction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Function 3): Keyword Extraction</h1>

<ul class="task-list">
<li><code>jieba.analyse.extract_tags(sentence,topK) # needs to first import jieba.analyse</code></li>
<li>
<code>setence</code>: the text to be extracted</li>
<li>
<code>topK</code>: To return several TF / IDF weights for the biggest keywords, the default value is 20</li>
</ul><p>Code sample (keyword extraction)</p>

<pre><code>https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py
</code></pre>

<h1>
<a name="user-content-using-other-dictionaries" class="anchor" href="#using-other-dictionaries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using Other Dictionaries</h1>

<p>It is possible to supply Jieba with your own custom dictionary, and there are also two dictionaries readily available for download:</p>

<ol class="task-list">
<li><p>You can employ a smaller dictionary for a smaller memory footprint:
<a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small</a></p></li>
<li><p>There is also a bigger file that has better support for traditional characters (繁體):
<a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big</a></p></li>
</ol><p>By default, an in-between dictionary is used, called <code>dict.txt</code> and included in the distribution.</p>

<p>In either case, download the file you want first, and then call <code>jieba.set_dictionary('data/dict.txt.big')</code> or just replace the existing <code>dict.txt</code>.</p>

<h1>
<a name="user-content-initialization" class="anchor" href="#initialization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initialization</h1>

<p>By default, Jieba employs lazy loading to only build the trie once it is necessary. This takes 1-3 seconds once, after which it is not initialized again. If you want to initialize Jieba manually, you can call:</p>

<pre><code>import jieba
jieba.initialize()  # (optional)
</code></pre>

<p>You can also specify the dictionary (not supported before version 0.28) :</p>

<pre><code>jieba.set_dictionary('data/dict.txt.big')
</code></pre>

<h1>
<a name="user-content-segmentation-speed" class="anchor" href="#segmentation-speed" aria-hidden="true"><span class="octicon octicon-link"></span></a>Segmentation speed</h1>

<ul class="task-list">
<li>1.5 MB / Second in Full Mode</li>
<li>400 KB / Second in Default Mode</li>
<li>Test Env: Intel(R) Core(TM) i7-2600 CPU @ 3.4GHz；《围城》.txt</li>
</ul><h1>
<a name="user-content-online-demo" class="anchor" href="#online-demo" aria-hidden="true"><span class="octicon octicon-link"></span></a>Online demo</h1>

<p><a href="http://jiebademo.ap01.aws.af.cm/">http://jiebademo.ap01.aws.af.cm/</a></p>

<p>(Powered by Appfog)</p></article></div>