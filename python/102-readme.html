<div class="announce instapaper_body md" data-path="README.md" id="readme"><article class="markdown-body entry-content" itemprop="mainContentOfPage"><p><a href="https://camo.githubusercontent.com/08d1070cf7eb1a9067d8a6c87a7c7aedda34c955/687474703a2f2f692e696d6775722e636f6d2f326e675a6f70532e6a7067" target="_blank"><img src="https://camo.githubusercontent.com/08d1070cf7eb1a9067d8a6c87a7c7aedda34c955/687474703a2f2f692e696d6775722e636f6d2f326e675a6f70532e6a7067" alt="Alt text" title="Screenshot" data-canonical-src="http://i.imgur.com/2ngZopS.jpg" style="max-width:100%;"></a></p>

<h2>
<a name="user-content-webcam-pulse-detector" class="anchor" href="#webcam-pulse-detector" aria-hidden="true"><span class="octicon octicon-link"></span></a>webcam-pulse-detector</h2>

<hr><p>UPDATE: Stand-alone (no dependancy) precompiled application now available!</p>

<ul class="task-list">
<li>Download for Windows 7 and 8: <a href="http://sourceforge.net/projects/webcampulsedetector/files/webcam-pulse-detector_win.zip/download">webcam-pulse-detector_win.zip</a> (42 Mb) </li>
<li>Download for Mac OSX 10.6 (and later): <a href="http://sourceforge.net/projects/webcampulsedetector/files/webcam-pulse-detector_mac.zip/download">webcam-pulse-detector_mac.zip</a> (21 Mb)</li>
<li>Debian/Ubuntu/Mint Linux: Coming very soon. For now, it is recommended that you run from source on the <code>no_openmdao</code> branch if you just want to test things out.</li>
</ul><p>The application can be run by simply executing the binary contained in the zip file for your platform.
This code can also be run from source by <a href="#running-from-source">following the instructions below</a>.</p>

<hr><p>A python code that detects the heart-rate of an individual using a common webcam or network IP camera. 
Tested on OSX 10.7, 10.8, 10.9, Ubuntu 13.04 (Ringtail), and Windows 7 &amp; 8.</p>

<p>Inspired by reviewing recent work on <a href="http://people.csail.mit.edu/mrub/vidmag/">Eulerian Video Magnification</a>, 
with motivation to implement something visually comparable (though not necessarily identical in formulation) to their
pulse detection examples using <a href="http://python.org/">Python</a> and <a href="http://opencv.org/">OpenCV</a> (see <a href="https://github.com/brycedrennan/eulerian-magnification">https://github.com/brycedrennan/eulerian-magnification</a> for a 
more general take on the offline post-processing methodology). 
This goal is comparable to those of a few previous efforts in this area 
(such as <a href="https://github.com/mossblaser/HeartMonitor">https://github.com/mossblaser/HeartMonitor</a>).</p>

<p>This code was developed at <a href="http://www.nasa.gov/centers/glenn">NASA Glenn Research Center</a> in 
support of <a href="http://openmdao.org/">OpenMDAO</a>, under the Aeronautical Sciences Project in NASA's 
<a href="http://www.aeronautics.nasa.gov/fap/">Fundamental Aeronautics Program</a>, as well as the Crew State Monitoring Element 
of the Vehicle Systems Safety Technologies Project, in NASAâ€™s 
<a href="http://www.aeronautics.nasa.gov/programs_avsafe.htm">Aviation Safety Program</a>.</p>

<h2>
<a name="user-content-how-it-works" class="anchor" href="#how-it-works" aria-hidden="true"><span class="octicon octicon-link"></span></a>How it works:</h2>

<p>This application uses <a href="http://opencv.org/">OpenCV</a> to find the location of the user's face, then isolate the forehead region. Data is collected
from this location over time to estimate the user's heart rate. This is done by measuring average optical
intensity in the forehead location, in the subimage's green channel alone (a better color mixing ratio may exist, but the 
blue channel tends to be very noisy). Physiological data can be estimated this way via a combination of 
<a href="http://en.wikipedia.org/wiki/Photoplethysmogram">photoplethysmology</a> and the optical absorption 
characteristics of (oxy-) haemoglobin (see <a href="http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-16-26-21434">http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-16-26-21434</a>). </p>

<p>With good lighting and minimal noise due to motion, a stable heartbeat should be 
isolated in about 15 seconds. Other physiological waveforms (such as 
<a href="http://en.wikipedia.org/wiki/Mayer_waves">Mayer waves</a>) should also be visible in the raw data stream.</p>

<p>Once the user's heart rate has been estimated, real-time phase variation associated with this 
frequency is also computed. This allows for the heartbeat to be exaggerated in the post-process frame rendering, 
causing the highlighted forehead location to pulse in sync with the user's own heartbeat.</p>

<p>Support for detection on multiple simultaneous individuals in a single camera's 
image stream is definitely possible, but at the moment only the information from one face 
is extracted for analysis.</p>

<p>The overall dataflow/execution order for the real-time signal processing looks like:</p>

<p><a href="https://camo.githubusercontent.com/52ffd5449ebd8bcba36ceecf0b094c49d71671ef/687474703a2f2f692e696d6775722e636f6d2f7853374f3855332e706e67" target="_blank"><img src="https://camo.githubusercontent.com/52ffd5449ebd8bcba36ceecf0b094c49d71671ef/687474703a2f2f692e696d6775722e636f6d2f7853374f3855332e706e67" alt="Alt text" title="Signal processing" data-canonical-src="http://i.imgur.com/xS7O8U3.png" style="max-width:100%;"></a></p>

<p>This signal processing design is implemented in the OpenMDAO assembly object defined in
<a href="lib/processors.py">lib/processors.py</a>.</p>

<p>The definition of each component block used can be found in the source 
files <a href="lib/imageProcess.py">lib/imageProcess.py</a>, <a href="lib/signalProcess.py">lib/signalProcess.py</a>, and 
<a href="lib/sliceops.py">lib/sliceops.py</a>. The <code>@bin</code> and <code>@bout</code> blocks in the above graph denote assembly-level input and 
output.</p>

<h2>
<a name="user-content-running-from-source" class="anchor" href="#running-from-source" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running from source:</h2>

<h3>
<a name="user-content-basic-requirements" class="anchor" href="#basic-requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Basic requirements:</h3>

<ul class="task-list">
<li><a href="http://python.org/">Python v2.7+</a></li>
<li>Numpy and Scipy </li>
<li>Pyserial (for serial port output support)</li>
<li>
<a href="http://opencv.org/">OpenCV v2.4+</a>, with the cv2 python bindings</li>
</ul><p>OpenCV is a powerful open-source computer vision library, with a convenient 
numpy-compatible interface in the cv2 bindings.</p>

<p><strong>If you want to run from source and modify UI or data output behavior, and make only minor changes 
to the signal processing, you can checkout and run the <code>no_openmdao</code> branch with no further dependancies.</strong>
This branch implements a 'flattened' version of the master branch's OpenMDAO assembly, but as a plain python object.</p>

<p>Both the <code>no_openmdao</code> branch and the precompiled binary applications contain support for real time serial port and UDP output of the estimated heart rate.</p>

<p>However, if you would like to make significant or exploratory changes to the signal processing code (eg. multichannel support, PCA/ICA data factorizations, better filters, etc.), you should run the master
branch with OpenMDAO support and build on what is already there (see instructions below).</p>

<h3>
<a name="user-content-requirements-for-master-branch-openmdao-support" class="anchor" href="#requirements-for-master-branch-openmdao-support" aria-hidden="true"><span class="octicon octicon-link"></span></a>Requirements for master branch (OpenMDAO support):</h3>

<ul class="task-list">
<li><a href="http://openmdao.org/">OpenMDAO v0.9+</a></li>
</ul><p>OpenMDAO is an open-source engineering framework that serves as a convenient 
environment to containerize the required real-time analysis, and 
allow for that analysis to be easily tweaked to specification and compared with alternative designs. 
Upon installation, OpenMDAO is bootstrapped into its own Python 
virtualenv, which must be activated before use (see the Quickstart section below). OpenMDAO requires python 2.6+, numpy, scipy, and matplotlib 
(see <a href="http://openmdao.org/docs/getting-started/requirements.html">http://openmdao.org/docs/getting-started/requirements.html</a>)</p>

<p><strong>Running Windows, completely new to Python, but still would like to hack on the <code>master</code> branch source code? Full instructions for getting started with all requirements needed to
run this code are available <a href="win_pythonxy.md">here</a></strong></p>

<h2>
<a name="user-content-quickstart" class="anchor" href="#quickstart" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quickstart:</h2>

<h3>
<a name="user-content-on-branch-no_openmdao" class="anchor" href="#on-branch-no_openmdao" aria-hidden="true"><span class="octicon octicon-link"></span></a>On branch <code>no_openmdao</code>:</h3>

<ul class="task-list">
<li>Simply run <code>get_pulse.py</code> in the top level directory.</li>
</ul><h3>
<a name="user-content-on-branch-master" class="anchor" href="#on-branch-master" aria-hidden="true"><span class="octicon octicon-link"></span></a>On branch <code>master</code>:</h3>

<ul class="task-list">
<li>Activate the openMDAO virtual python environment in a command or terminal window. On Linux and OSX, this is done by
running (note the period):</li>
</ul><pre><code>. OpenMDAO/bin/activate
</code></pre>

<p>Or on Windows:</p>

<pre><code>OpenMDAO\Scripts\activate
</code></pre>

<ul class="task-list">
<li>In the activated environment, navigate to the downloaded source directory, and run get_pulse.py to start the application</li>
</ul><pre><code>python get_pulse.py
</code></pre>

<ul class="task-list">
<li>If there is an error, try running <code>test_webcam.py</code> in the same directory to check if your openCV installation and webcam can be made to work
with this application.</li>
</ul><h2>
<a name="user-content-usage-notes" class="anchor" href="#usage-notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage notes:</h2>

<ul class="task-list">
<li>When run, a window will open showing a stream from your computer's webcam</li>
<li>When a forehead location has been correctly detected and isolated, the user should press "S" on their 
keyboard to lock this location, and remain as still as possible (the camera 
stream window must have focus for the click to register). This freezes the acquisition location in place. This lock can
be released by pressing "S" again.</li>
<li>To view a stream of the measured data as it is gathered, press "D". To hide this display, press "D" again.</li>
<li>The data display shows three data traces, from top to bottom: 

<ol class="task-list">
<li>(top) raw optical intensity</li>
<li>(bottom) Power spectral density, with local maxima indicating the heartrate (in beats per minute). </li>
</ol>
</li>
<li>With consistent lighting and minimal head motion, a stable heartbeat should be 
isolated in about 15 seconds. A count-down is shown in the image frame.</li>
</ul><h2>
<a name="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span class="octicon octicon-link"></span></a>TODO:</h2>

<ul class="task-list">
<li>There have been some requests for a video demo</li>
<li>Instead of processing using the green channel alone, it is likely that some fixed combination of the statistics of the
R,G,B channels could instead be optimal (though I was unable to find a simple combination that was better than green
alone). If so, the mixing ratios might be determinable from the forward projection matrices of PCA or ICA operators 
computed on a set of mean value R,G, and B data gathered over a trial data set (and verified with different individuals 
under different lighting conditions).</li>
<li>Support for multiple individuals</li>
<li>Smoother tracking of data from foreheads, perhaps by buffering and registering or inverse-transforming image subframes</li>
</ul></article></div>