<div class="announce instapaper_body md" data-path="README.md" id="readme"><article class="markdown-body entry-content" itemprop="mainContentOfPage"><h1>
<a name="user-content-portia" class="anchor" href="#portia" aria-hidden="true"><span class="octicon octicon-link"></span></a>portia</h1>

<p>Visual scraping for Scrapy.</p>

<h1>
<a name="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h1>

<p>Portia is a tool for visually scraping web sites without any programming knowledge. Just annotate web pages with a point and click editor to indicate what data you want to extract, and portia will learn how to scrape similar pages
from the site.</p>

<p>Portia has a web based UI served by a <a href="https://twistedmatrix.com">Twisted</a> server, so you can install it on almost any modern platform.</p>

<h1>
<a name="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Requirements</h1>

<ul class="task-list">
<li>Python 2.7</li>
<li>Works on Linux, Windows, Mac OSX, BSD</li>
<li>Supported browsers: Latest versions of Chrome (recommended) or Firefox</li>
</ul><h1>
<a name="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prerequisites</h1>

<p>You might need to run the following commands to install the required tools &amp; libraries before building portia:</p>

<pre><code>apt-get install python-pip python-dev libxml2-dev libxslt1-dev libffi-dev libssl-dev
pip install virtualenv
</code></pre>

<h1>
<a name="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Installation</h1>

<p>The recommended way to install dependencies is to use <strong>virtualenv</strong>:</p>

<pre><code>virtualenv YOUR_ENV_NAME --no-site-packages
</code></pre>

<p>and then do:</p>

<pre><code>source YOUR_ENV_NAME/bin/activate
cd slyd
pip install -r requirements.txt
</code></pre>

<p>As <code>slybot</code> is a <code>slyd</code> dependency, it will also get installed.</p>

<p><strong>Note:</strong> you may need to use <code>sudo</code> or <code>pip --user</code> if you get permissions problems while installing.</p>

<h1>
<a name="user-content-running-portia" class="anchor" href="#running-portia" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running portia</h1>

<p>First, you need to start the ui and create a project. Run <strong>slyd</strong> using:</p>

<pre><code>cd slyd
twistd -n slyd
</code></pre>

<p>and point your browser to: <code>http://localhost:9001/static/main.html</code></p>

<p>Choose the site you want to scrape and create a project. Every project is created with a default spider named after the domain of the site you are scraping. When you are ready, you can run your project with <strong>slybot</strong> to do the actual crawling/extraction.</p>

<p>Projects created with <strong>slyd</strong> can be found at:</p>

<pre><code>slyd/data/projects
</code></pre>

<p>To run one of those projects use:</p>

<pre><code>portiacrawl project_path spidername
</code></pre>

<p>Where <code>spidername</code> should be one of the project spiders. If you don't remember the name of the spider, just use:</p>

<pre><code>portiacrawl project_path
</code></pre>

<p>and you will get the list of spiders for that project.</p>

<p>Portia spiders are ultimately <a href="http://scrapy.org">Scrapy</a> spiders. You can pass <strong>scrapy</strong> spider arguments when running them with <code>portiacrawl</code> by using the <code>-a</code> command line option. A custom settings module may also be specified using the <code>--settings</code> command line option. Please refer to the <a href="http://doc.scrapy.org/en/latest">scrapy documentation</a> for details on arguments and settings.</p>

<h1>
<a name="user-content-running-portia-with-vagrant" class="anchor" href="#running-portia-with-vagrant" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running portia with <a href="http://www.vagrantup.com">vagrant</a>
</h1>

<p>This is probably the easiest way to install and run portia.</p>

<p>First, you need to get:</p>

<ul class="task-list">
<li>Vagrant: <a href="http://www.vagrantup.com/downloads.html">http://www.vagrantup.com/downloads.html</a>
</li>
<li>VirtualBox: <a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a>
</li>
</ul><p>After that <code>cd</code> into the repo directory and run:</p>

<pre><code>vagrant up
</code></pre>

<p>This will setup and start an ubuntu virtual machine, build portia and launch the <code>slyd</code> server for you. Just point your browser to <code>http://localhost:8000/static/main.html</code> after vagrant has finished the whole process (you should see <code>default: slyd start/running, process XXXX</code> in your console) and you can start using portia. You can stop the server with <code>vagrant suspend</code> or <code>vagrant halt</code>.</p>

<p>The repository directory is shared with the VM, so you don't need to do anything special to keep it in sync. You can <strong>ssh</strong> into the virtual machine by running <code>vagrant ssh</code>. The repo dir will be mounted at <code>/vagrant</code> in the VM. Please note that you <strong>need to ssh into the VM to run the <code>portiacrawl</code> script</strong>.</p>

<h1>
<a name="user-content-repository-structure" class="anchor" href="#repository-structure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Repository structure</h1>

<p>There are two main components in this repository, <strong>slyd</strong> and <strong>slybot</strong>:</p>

<h3>
<a name="user-content-slyd" class="anchor" href="#slyd" aria-hidden="true"><span class="octicon octicon-link"></span></a>slyd</h3>

<p>The visual editor used to create your scraping projects.</p>

<h3>
<a name="user-content-slybot" class="anchor" href="#slybot" aria-hidden="true"><span class="octicon octicon-link"></span></a>slybot</h3>

<p>The Python web crawler that performs the actual site scraping. It's implemented on top of the <a href="http://scrapy.org">Scrapy</a> web crawling
framework and the <a href="https://github.com/scrapy/scrapely">Scrapely</a> extraction library. It uses projects created with <strong>slyd</strong> as input.</p></article></div>